{"cells":[{"metadata":{},"cell_type":"markdown","source":"# <center>CNN on COVID-19 CT lungs scans</center>\n## <div align=right>Made by Ihor Markevych</div>"},{"metadata":{},"cell_type":"markdown","source":"[My Medium article](https://medium.com/@ih.markevych/4294e29b72b?source=friends_link&sk=ce5ebe4837e469ce0ce4f7145b3db08d)."},{"metadata":{},"cell_type":"markdown","source":"Since we have a small dataset, we will be using validation set as test set. However, we won't be setting seed, which will allow us to have different subset of data each time and therefore it will give more robust evaluation of performance together with saving data for training.\n  \n----------"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":false},"cell_type":"code","source":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\n\nimport tensorflow as tf\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport os\n\nimport sklearn.metrics","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"DIR = '/kaggle/input/covidct/'\nSUBDIR_POS = 'CT_COVID/'\nSUBDIR_NEG = 'CT_NonCOVID/'\nprint(f'Positive samples: {len(os.listdir(DIR + SUBDIR_POS))}.')\nprint(f'Negative samples: {len(os.listdir(DIR + SUBDIR_NEG))}.')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"scrolled":false},"cell_type":"code","source":"im = cv2.imread('/kaggle/input/covidct/CT_COVID/2020.03.20.20037325-p23-122.png', 0) / 255\nplt.imshow(im, cmap='gray', vmin=0, vmax=1) \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"EPOCHS = 40\nBATCH_SIZE = 64\nOPTIMIZER = tf.keras.optimizers.Adam(learning_rate=0.001, decay=0.001 / EPOCHS)\nimg_height, img_width = 248, 248\nes = EarlyStopping(monitor='val_acc', mode='max',\n                   verbose=1, \n                   patience=10, restore_best_weights=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"# https://stackoverflow.com/questions/42443936/keras-split-train-test-set-when-using-imagedatagenerator\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    horizontal_flip=True,\n    rotation_range=5,\n    width_shift_range=0.05,\n    height_shift_range=0.05,\n    shear_range=0.05,\n    zoom_range=0.05,\n    validation_split=0.2) \n\ntrain_generator = train_datagen.flow_from_directory(\n    DIR,\n    target_size=(img_height, img_width),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    color_mode=\"grayscale\",\n    subset='training') \n\nvalidation_generator = train_datagen.flow_from_directory(\n    DIR, \n    target_size=(img_height, img_width),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    color_mode=\"grayscale\",\n    subset='validation') ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"def create_model():\n    model = Sequential([\n        Conv2D(16, 1, padding='same', activation='relu', input_shape=(img_height, img_width, 1)),\n        MaxPooling2D(),\n        Conv2D(32, 3, padding='same', activation='relu'),\n        MaxPooling2D(),\n        Conv2D(64, 5, padding='same', activation='relu'),\n        MaxPooling2D(),\n        Conv2D(64, 5, padding='same', activation='relu'),\n        MaxPooling2D(),\n        \n        Flatten(),\n        Dense(128, activation='relu'),\n        Dropout(0.4),\n        Dense(64, activation='relu'),\n        Dropout(0.5),\n        Dense(8, activation='relu'),\n        Dropout(0.3),\n        Dense(1, activation='sigmoid')\n    ])\n\n    model.compile(optimizer=OPTIMIZER,\n                  loss='binary_crossentropy',\n                  metrics=['accuracy', 'Precision', 'Recall'])\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"model = create_model()\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"hist = model.fit_generator(\n        train_generator,\n        steps_per_epoch = train_generator.samples // BATCH_SIZE,\n        validation_data = validation_generator, \n        validation_steps = validation_generator.samples // BATCH_SIZE,\n        epochs = EPOCHS,\n        verbose = 0, \n        callbacks=[es])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"plt.title('Accuracy')\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\nplt.title('Loss')\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\nplt.title('Recall')\nplt.plot(hist.history['Recall'])\nplt.plot(hist.history['val_Recall'])\nplt.ylabel('recall')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\nplt.title('Precision')\nplt.plot(hist.history['Precision'])\nplt.plot(hist.history['val_Precision'])\nplt.ylabel('recall')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = (model.predict_generator(validation_generator) > 0.5).astype(int)\ny_true = validation_generator.classes\n\nfor name, value in zip(model.metrics_names, model.evaluate_generator(validation_generator)):\n    print(f'{name}: {value}')\n    \nprint(f'F1 score: {sklearn.metrics.f1_score(y_true, y_pred)}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"pd.DataFrame(sklearn.metrics.confusion_matrix(y_true, y_pred), \n             columns=['pred no covid', 'pred covid'], \n             index=['true no covid', 'true covid'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Saving model"},{"metadata":{"trusted":true,"scrolled":false},"cell_type":"code","source":"model_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\nmodel.save_weights(\"model.h5\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Underfit / overfit\n\nWe observe that model is not overfitting, as validation loss is better than training. This is explained by dropout - as dropout is not included in prediction, model is having full potential and generally making better predictions. However, if we add additional neurons, layers or filters, model will start performing badly, therefore, we can make a conclusion that we don't have removable bias in the model.  \n  \nGeneralizability of the model is limited, since the train set size is only around 600 images. Additionally, it is possible to include images of other pneumonia-like ilnesses and see if it is possible to build a model that would distinguish them correctly. It may be that this task would be significantly harder and we would need to go with just two classes - healthy person and coronavirus/other ilness, as differentiating of pneumonia/coronavirus may be incredibly hard from just CT scans.  \n  \nHow to make better model - we can take pretrained model (or pretrain it by ourselves) that is capable of diagnosing similar illnesses and apply transfer learning with COVID-19 dataset. CT lungs scans of person with pneumonia can be quite close to CT scans of person with coronavirus, so, such dataset can be used."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"markdown","source":"Since we are not setting seed, performance may slightly vary, but accuracy is in range between 70-80%. If we continue training, we observe overfitting, therefore we stop our training after 40 epochs."},{"metadata":{"trusted":true,"scrolled":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}